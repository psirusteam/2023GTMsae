---
title: "Modelos lineales y modelos de vinculo logístico en R y STAN"
subtitle: "CEPAL - Unidad de Estadísticas Sociales"
author: "Andrés Gutiérrez - Stalyn Guerrero"
format: html
project:
  type: website
  output-dir: docs
---

```{r setup, include=FALSE, message=FALSE, error=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = T)
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  cache.path = "0Recursos/2Modelos/",
  fig.path = "0Recursos/2Modelos/"
)
library(printr)
library(kableExtra)
library(tidyverse)
tba <- function(dat, cap = NA){
  kable(dat,
      format = "html", digits =  4,
      caption = cap) %>% 
     kable_styling(bootstrap_options = "striped", full_width = F)%>%
         kable_classic(full_width = F, html_font = "Arial Narrow")
}

```

## Modelos lineales.

La regresión lineal es la técnica básica del análisis econométrico. Mediante dicha técnica tratamos de determinar relaciones de dependencia de tipo lineal entre una variable dependiente o endógena, respecto de una o varias variables explicativas o exógenas.

$$
\begin{eqnarray*}
\left[\begin{array}{c}
Y_{1}\\
Y_{2}\\
\vdots\\
Y_{n}
\end{array}\right] & = & \left[\begin{array}{cccc}
1 & x_{11} & \cdots & x_{1k}\\
1 & x_{21} & \cdots & x_{2k}\\
\vdots & \vdots & \ddots & \vdots\\
1 & x_{n1} & \cdots & x_{nk}
\end{array}\right]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{k}
\end{array}\right]+\left[\begin{array}{c}
\epsilon_{1}\\
\epsilon_{2}\\
\vdots\\
\epsilon_{n}
\end{array}\right]
\end{eqnarray*}
$$ escrita de forma general sería

$$
\begin{align*}
\boldsymbol{Y} & = & \boldsymbol{X\beta} + \boldsymbol{\epsilon} \\
E\left(\boldsymbol{Y}\mid \boldsymbol{X}\right) & = & \boldsymbol{XB}
\end{align*}
$$

donde $\boldsymbol{\epsilon} \sim N\left( 0, \sigma^2\boldsymbol{I}_n\right)$ y el estimador de $B$ esta dado por:

$$
\boldsymbol{\hat{B}}=\boldsymbol{\left(X^{T}X\right)^{-1}X^{T}Y}
$$

## Modelos lineales bayesiano.

En primer lugar, nótese que el interés particular recae en la distribución del vector de $n$ variables aleatorias $\mathbf{Y}=(Y_1\ldots,Y_n)'$ condicional a la matriz de variables auxiliares $\mathbf{X}$ e indexada por el vector de parámetros de interés $\boldsymbol{\beta}=(\beta_0,\ldots,\beta_k)'$ dada por $p(\mathbf{Y} \mid \boldsymbol{\beta},\mathbf{X})$.

El modelo básico y clásico asume que la verosimilitud para las variables de interés es

$$
\begin{equation*}
\mathbf{Y} \mid \boldsymbol{\beta},\sigma^2,\mathbf{X}\sim N_n(\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I}_n)
\end{equation*}
$$

en donde $\mathbf{I}_n$ denota la matriz identidad de orden $n\times n$. Por supuesto, el modelo normal no es el único que se puede postular como verosimilitud para los datos.

### Parámetros dependientes.

Los parámetros de interés son $\boldsymbol{\beta}$ y $\sigma^2$ y su distribuciones previa conjunta se supone que está dada por

$$
\begin{equation*}
p(\boldsymbol{\beta},\sigma^2)=p(\boldsymbol{\beta} \mid \sigma^2)p(\sigma^2)
\end{equation*}
$$

Específicamente, la distribución previa del parámetro $\boldsymbol{\beta}$ condicionada a $\sigma^2$ es informativa y está regida por la siguiente estructura probabilística

$$
\begin{equation*}
\boldsymbol{\beta} \mid \sigma^2 \sim N_q(\mathbf{b},\sigma^2\mathbf{B})
\end{equation*}
$$

en donde $\mathbf{b}$ es un vector de medias y $\mathbf{B}$ es una matriz de varianzas simétrica y definida positiva. Por otro lado, la distribución previa del parámetro $\sigma^2$ también se considera informativa y dada por

$$
\begin{equation*}
\sigma^2 \sim Inversa-Gamma\left( \frac{n_0}{2}, \frac{n_0\sigma^2_0}{2} \right)
\end{equation*}
$$

La distribución posterior conjunta de los parámetros de interés $\boldsymbol{\beta},\sigma^2$ está dada por

$$
\begin{align}
p(\boldsymbol{\beta},\sigma^2 \mid \mathbf{Y},\mathbf{X})
&=(\sigma^2)^{-q/2}
\exp\left\{-\frac{1}{2\sigma^2}(\boldsymbol{\beta}-\mathbf{b}_q)'\mathbf{B}_q^{-1}(\boldsymbol{\beta}-\mathbf{b}_q)\right\}
\notag
\\ &\hspace{3cm} \times
(\sigma^2)^{-n_1/2-1}
\exp\left\{-\frac{n_1\sigma^2_1}{2\sigma^2}\right\}
\end{align}
$$ donde

$$
\begin{align*}
\mathbf{B}_q &= \left(\mathbf{B}^{-1}+\mathbf{X}'\mathbf{X}\right)^{-1}\\
\mathbf{b}_q &=\mathbf{B}_q\left(\mathbf{B}^{-1}\mathbf{b}+\mathbf{X}'\mathbf{Y}\right)
\end{align*}
$$

y además

$$
\begin{align*}
n_1&=n_0+n\\
n_1\sigma^2_1&=
n_0\sigma^2_0+(\mathbf{Y}-\mathbf{X}\mathbf{b}_q)'\mathbf{Y}+(\mathbf{b}-\mathbf{b}_q)'\mathbf{B}^{-1}\mathbf{b}
\end{align*}
$$

donde $n_0$ denota el número de datos previos. La distribución posterior conjunta de los parámetros de interés tiene la forma de la distribución Normal-Gamma.

La distribución posterior del vector de parámetros $\boldsymbol{\beta}$ condicionada a $\sigma^2,\mathbf{Y},\mathbf{X}$ es

$$
\begin{equation*}
\boldsymbol{\beta} \mid \sigma^2,\mathbf{Y},\mathbf{X} \sim N_q(\mathbf{b}_q,\sigma^2\mathbf{B}_q)
\end{equation*}
$$

La distribución posterior del parámetro $\sigma^2$ condicionada es

$$
\begin{equation*}
\sigma^2 \mid \mathbf{Y},\mathbf{X} \sim Inversa-Gamma\left(\frac{n_1}{2},\frac{\sigma^2_1}{2}\right)
\end{equation*}
$$

### Parámetros independientes

En esta ocasión se considera que los parámetros son independientes previa; es decir que la distribución previa conjunta está dada por

$$
\begin{equation*}
p(\boldsymbol{\beta},\sigma^2)=p(\boldsymbol{\beta})p(\sigma^2)
\end{equation*}
$$

Como es natural, la distribución previa del vector de parámetros $\boldsymbol{\beta}$ es normal, aunque esta vez la matriz de varianzas no va a depender del otro parámetro $\sigma^2$, por lo tanto se tiene que

$$
\begin{equation*}
\boldsymbol{\beta} \sim N_q(\mathbf{b},\mathbf{B})
\end{equation*}
$$

Igualmente, el parámetro $\sigma^2$ no depende de $\boldsymbol{\beta}$ y es posible asignarle la siguiente distribución previa

$$
\begin{equation*}
\sigma^2\sim Inversa-Gamma\left(\frac{n_0}{2},\frac{n_0\sigma^2_0}{2}\right)
\end{equation*}
$$

la distribución posterior conjunta de $\boldsymbol{\beta}$ y $\sigma^2$ puede ser escrita como

$$
\begin{align}
p(\boldsymbol{\beta},\sigma^2 \mid \mathbf{Y},\mathbf{X})&\propto p(\mathbf{Y} \mid \boldsymbol{\beta},\sigma^2)p(\boldsymbol{\beta})p(\sigma^2)\notag \\
&\propto (\sigma^2)^{-n/2} \exp\left\{-\frac{1}{2\sigma^2}\left(Q(\boldsymbol{\beta})+S^2_e\right)\right\}\notag\\
&\times
\exp\left\{-\frac{1}{2}(\boldsymbol{\beta}-\mathbf{b})'\mathbf{B}^{-1}(\boldsymbol{\beta}-\mathbf{b})\right\}
(\sigma^2)^{-n_0/2-1} \exp\left\{-\frac{n_0\sigma^2_0}{2\sigma^2}\right\}\notag\\
&=(\sigma^2)^{-\frac{n+n_0}{2}-1}
\exp\left\{-\frac{1}{2\sigma^2}\left[Q(\boldsymbol{\beta})+S^2_e+n_0\sigma^2_0\right]\right\} \notag \\
&\times
\exp\left\{-\frac{1}{2}(\boldsymbol{\beta}-\mathbf{b})'\mathbf{B}^{-1}(\boldsymbol{\beta}-\mathbf{b})\right\}
\end{align}
$$

La distribución posterior del parámetro $\boldsymbol{\beta}$ condicionado a $\sigma^2,\mathbf{Y},\mathbf{X}$ es

$$
\begin{equation*}
\boldsymbol{\beta} \mid \sigma^2,\mathbf{Y},\mathbf{X} \sim N_q(\mathbf{b}_q,\mathbf{B}_q)
\end{equation*}
$$

donde

$$
\begin{align*}
\mathbf{B}_q &= \left(\mathbf{B}^{-1}+\frac{1}{\sigma^2}\mathbf{X}'\mathbf{X}\right)^{-1}\\
\mathbf{b}_q &=\mathbf{B}_q\left(\mathbf{B}^{-1}\mathbf{b}+\frac{1}{\sigma^2}\mathbf{X}'\mathbf{Y}\right)
\end{align*}
$$

La distribución posterior del parámetro $\sigma^2$ condicionado a $\boldsymbol{\beta},\mathbf{Y},\mathbf{X}$ es

$$
\begin{equation*}
\sigma^2 \mid \boldsymbol{\beta},\mathbf{Y},\mathbf{X} \sim Inversa-Gamma\left( \frac{n_1}{2}, \frac{n_1\sigma_{\boldsymbol{\beta}}^2}{2}  \right)
\end{equation*}
$$

donde $n_1=n+n_0$,\
$$
\begin{align*}
n_1\sigma_{\boldsymbol{\beta}}^2&=&Q(\boldsymbol{\beta})+S^2_e+n_0\sigma^2_0\\
Q(\boldsymbol{\beta})&=&(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})'(\mathbf{X}'\mathbf{X})(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})\\
S^2_e&=&(\mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}})'(\mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}})
\end{align*}
$$

y $\sigma^2_0$ es una estimación previa del parámetro de interés $\sigma^2$.

### Práctica en **STAN** (Modelo lineal simple)

-   ENCUESTA CONTINUA DE HOGARES (ECH) 2020

```{r, message=FALSE, echo=TRUE, warning=FALSE}
library(tidyverse)
encuesta <- readRDS("../Data/encuestaGTM14N.rds") %>% 
  filter(edad > 18)
statelevel_predictors_df <- readRDS("../Data/statelevel_predictors_df.rds")
datalm <- encuesta %>% 
  transmute(
  dam = str_pad(dam_ee, pad = "0", width = 2), 
  ingcorte) %>% group_by(dam) %>% 
  summarise(  Promedio = log(mean(ingcorte))) %>% 
    left_join(statelevel_predictors_df, by = "dam")
tba(datalm)
```

Diagrama de dispersión por las variables

```{r, message=FALSE, echo=FALSE, warning=FALSE, out.height = "150%"}
datalm %>% select(dam, Promedio,
                  tiene_internet,
                  rezago_escolar ,
                  tiene_gas, 
                  alfabeta )%>%
  gather(key = "Variable",value = "X",
                  -dam,-Promedio) %>% 
ggplot(data = ., aes(y = Promedio, x = X )) +
  geom_point() + 
  facet_grid(.~Variable, scales = "free") + 
  theme_bw(20)

```

Creando código de `STAN`

```{r, eval=FALSE}
data {
  int<lower=0> n;   // Número de observaciones
  vector[n] x;      // Variable predictora
  vector[n] y;      // Variable respuesta
}
parameters {
  real b0;            // Intercepto
  real b1;            // Pendiente
  real<lower=0> sigma2;  
}
transformed parameters{
  real<lower=0> sigma;
  sigma = sqrt(sigma2);
}

model {
  b0 ~ normal(0, 1000);
  b1 ~ normal(0, 1000);
  sigma2 ~ inv_gamma(0.0001, 0.0001);
  y ~ normal(b0 + b1*x, sigma);  // likelihood
}
generated quantities {
    real ypred[n];                    // vector de longitud n
    ypred = normal_rng(b0 + b1*x, sigma);

}

```

Preparando el código de `STAN`

```{r, eval=TRUE}
library(cmdstanr)
## Definir el modelo
#file.edit("../Data/modelosStan/7ModeloLm.stan")
fitLm1 <- cmdstan_model(stan_file = "../Data/modelosStan/7ModeloLm.stan") 
```

Organizando datos para `STAN`

```{r}
sample_data <- list(n = nrow(datalm),
                    x = datalm$tiene_internet ,
                    y = datalm$Promedio)
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
model_fitLm1 <- fitLm1$sample(data = sample_data, 
                 chains = 4,
                 parallel_chains = 4,
                 num_warmup = 1000,
                 num_samples = 1000,
                 seed = 1234,
                 refresh = 1000)
```

La estimación del parámetro $B$ es:

```{r}
model_fitLm1$summary(variables = c("b0","b1","sigma2","sigma")) %>%
  select(variable:q95) %>% tba()
```

```{r}
library(posterior)
library(bayesplot)
library(patchwork)
(mcmc_hist_by_chain(model_fitLm1$draws("b0")) +
mcmc_areas(model_fitLm1$draws("b0")))/ 
mcmc_trace(model_fitLm1$draws("b0")) 

```

```{r}
(mcmc_hist_by_chain(model_fitLm1$draws("b1")) +
mcmc_areas(model_fitLm1$draws("b1")))/ 
mcmc_trace(model_fitLm1$draws("b1")) 

```

```{r}
(mcmc_hist_by_chain(model_fitLm1$draws("sigma2")) +
mcmc_areas(model_fitLm1$draws("sigma2")))/ 
mcmc_trace(model_fitLm1$draws("sigma2")) 

```

```{r}
y_pred_B <- model_fitLm1$draws(variables = "ypred", format = "matrix")
rowsrandom <- sample(nrow(y_pred_B), 100)
y_pred2 <- y_pred_B[rowsrandom, ]
ppc_dens_overlay(y = as.numeric(datalm$Promedio), y_pred2)
```

### Práctica en **STAN** (Modelo lineal múltiple)

Creando código de `STAN`

```{r, eval=FALSE}
data {
  int<lower=0> n;   // Número de observaciones
  int<lower=0> K;   // Número de predictores
  matrix[n, K] x;   // Matrix de predictores
  vector[n] y;      // Vector respuesta
}
parameters {
  vector[K] beta;       // coefficients for predictors
  real<lower=0> sigma2;  // error scale
}
transformed parameters{
  real<lower=0> sigma;
  sigma = sqrt(sigma2);
}

model {
  to_vector(beta) ~ normal(0, 10000);
  sigma2 ~ inv_gamma(0.0001, 0.0001);
  y ~ normal(x * beta, sigma);  // likelihood
}
generated quantities {
    real ypred[n];                    // vector de longitud n
    ypred = normal_rng(x * beta, sigma);
}

```

Preparando el código de `STAN`

```{r, eval=TRUE}
# file.edit("../Data/modelosStan/8ModeloLm.stan")
fitLm2 <- cmdstan_model(stan_file = "../Data/modelosStan/8ModeloLm.stan") 
```

Organizando datos para `STAN`

```{r}
Xdat <- model.matrix(Promedio ~ tiene_internet+
                  rezago_escolar +
                  eliminar_basura + 
                  alfabeta  , data = datalm)
tba(Xdat)
```

```{r}
sample_data <- list(n = nrow(datalm),
                    K = ncol(Xdat),
                    x = as.matrix(Xdat),
                    y = datalm$Promedio)
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
model_fitLm2 <- fitLm2$sample(data = sample_data, 
                 chains = 4,
                 parallel_chains = 4,
                 seed = 1234,
                 refresh = 1000)
```

La estimación del parámetro $B$ es:

```{r}
model_fitLm2$summary(variables = c("beta","sigma2")) %>%
  select(variable:q95) %>% tba()
```

```{r}
(mcmc_hist_by_chain(model_fitLm2$draws("beta[1]")) +
mcmc_areas(model_fitLm2$draws("beta[1]")))/ 
mcmc_trace(model_fitLm2$draws("beta[1]")) 

```

```{r}
(mcmc_hist_by_chain(model_fitLm2$draws("beta[2]")) +
mcmc_areas(model_fitLm2$draws("beta[2]")))/ 
mcmc_trace(model_fitLm2$draws("beta[2]")) 
```

```{r}
(mcmc_hist_by_chain(model_fitLm2$draws("beta[3]")) +
mcmc_areas(model_fitLm2$draws("beta[3]")))/ 
mcmc_trace(model_fitLm2$draws("beta[3]")) 
```

```{r}
(mcmc_hist_by_chain(model_fitLm2$draws("beta[4]")) +
mcmc_areas(model_fitLm2$draws("beta[4]")))/ 
mcmc_trace(model_fitLm2$draws("beta[4]")) 
```

```{r}
(mcmc_hist_by_chain(model_fitLm1$draws("sigma2")) +
mcmc_areas(model_fitLm1$draws("sigma2")))/ 
mcmc_trace(model_fitLm1$draws("sigma2")) 

```

```{r}
y_pred_B <- model_fitLm2$draws(variables = "ypred", format = "matrix")
rowsrandom <- sample(nrow(y_pred_B), 100)
y_pred2 <- y_pred_B[rowsrandom,]
ppc_dens_overlay(y = as.numeric(datalm$Promedio), y_pred2)
```

## Modelo Bernoulli con vínculo logístico

Este caso es típico en donde la variable respuesta sólo toma dos valores, uno en caso de un evento exitoso y cero, cuando se presenta un fracaso. Se supone que $\mathbf{Y}=\{Y_1, \ldots, Y_n\}$ es un conjunto de variables aleatorias intercambiables cada una con distribución bernoulli de parámetro $\theta_i$, y se quiere estudiar la relación entre $\theta_i$ y las variables auxiliares $\mathbf{X}_i$ por medio de la función de enlace $g(\theta_i)=\mathbf{X}_i'\boldsymbol{\beta}$. Aquí consideramos la función de enlace logística

$$
\begin{equation}
\eta_i=g(\theta_i)=logit(\theta_i)=\log\left(\frac{\theta_i}{1-\theta_i}\right)
\end{equation}
$$

fácilmente se encuentra que la función inversa para $g(\cdot)$, está dada por

$$
\begin{equation*}
\theta_i=g^{-1}(\eta_i)=\frac{\exp(\eta_i)}{1+\exp(\eta_i)}
\end{equation*}
$$

Notando que $\eta_i=\mathbf{X}_i'\boldsymbol{\beta}$ y siguiendo con el modelamiento, se tiene que la verosimilitud de los datos está dada por

$$
\begin{align}
p(\mathbf{Y}\mid \boldsymbol{\theta})&=\prod_{i=1}^n\theta_i^{y_i}(1-\theta_i)^{1-y_i} \notag \\
p(\mathbf{Y}\mid \boldsymbol{\beta})&=\prod_{i=1}^n\left(\frac{\exp(\mathbf{X}_i'\boldsymbol{\beta})}{1+\exp(\mathbf{X}_i'\boldsymbol{\beta})}\right)^{y_i}
\left(1-\left(\frac{\exp(\mathbf{X}_i'\boldsymbol{\beta})}{1+\exp(\mathbf{X}_i'\boldsymbol{\beta})}\right)\right)^{1-y_i}
\end{align}
$$

Suponga que la distribución a previa para $\boldsymbol{\beta}$ está regida por la siguiente estructura probabilística

$$
\begin{equation*}
\boldsymbol{\beta}\sim N_q(\mathbf{b},\mathbf{B})
\end{equation*}
$$

De esta manera, la distribución a posterior toma la siguiente forma.

$$
\begin{align*}
p(\boldsymbol{\beta} \mid \mathbf{Y}, \mathbf{X})&\propto
\prod_{i=1}^n\left(\frac{\exp(\mathbf{X}_i'\boldsymbol{\beta})}{1+\exp(\mathbf{X}_i'\boldsymbol{\beta})}\right)^{y_i}
\left(1-\left(\frac{\exp(\mathbf{X}_i'\boldsymbol{\beta})}{1+\exp(\mathbf{X}_i'\boldsymbol{\beta})}\right)\right)^{1-y_i}\\
&\hspace{2cm}\times
\exp\left\{\frac{-1}{2}(\boldsymbol{\beta}-\mathbf{b})'\mathbf{B}^{-1}(\boldsymbol{\beta}-\mathbf{b})\right\}
\end{align*}
$$

La anterior expresión no tiene una forma cerrada y no es sencillo, en primera instancia, simular observaciones u obtener inferencias a posterior.

```{r}
library(forcats)
datalogit <- encuesta %>% 
  transmute(
    dam = str_pad(dam_ee, pad = "0", width = 2), 
    pobreza = ifelse(ingcorte < lp, 1, 0),
    sexo = as_factor(sexo),  
  anoest = case_when(
    edad < 5 | is.na(anoest)   ~ "98"  , #No aplica
    anoest == 99 ~ "99", #NS/NR
    anoest == 0  ~ "1", # Sin educacion
    anoest %in% c(1:6) ~ "2",       # 1 - 6
    anoest %in% c(7:12) ~ "3",      # 7 - 12
    anoest > 12 ~ "4" ),
  
  edad = case_when(
    edad < 15 ~ "1",
    edad < 30 ~ "2",
    edad < 45 ~ "3",
    edad < 65 ~ "4",
    TRUE ~ "5"),
  ) %>% 
    filter(dam =="02", anoest %in% c(1:4))
tba(head(datalogit, 10))
```

### Práctica en **STAN**

Creando código de `STAN`

```{r, eval=FALSE}
data {
  int<lower=0> n;   // Número de observaciones
  int<lower=0> K;   // Número de predictores
  matrix[n, K] x;   // Matrix de predictores
  int y;      // Vector respuesta
}
parameters {
  vector[K] beta;       // coefficients for predictors
}
transformed parameters {
    vector[n] inv_eta;
   inv_eta = inv_logit(x * beta);
}

model {
  to_vector(beta) ~ normal(0, 10000);
  y ~ bernoulli(inv_eta);  // likelihood
}
generated quantities {
    real ypred[n];                    // vector de longitud n
    ypred = bernoulli_rng(inv_eta);
}


```

Preparando el código de `STAN`

```{r, eval=TRUE}
# file.edit("../Data/modelosStan/9ModeloLogit.stan")
fitLosgit1 <- cmdstan_model(stan_file = "../Data/modelosStan/9ModeloLogit.stan") 
```

Organizando datos para `STAN`

```{r}
Xdat <- model.matrix(pobreza ~ sexo + anoest + edad,
                     data = datalogit)
tba(head(Xdat, 10))
```

```{r}
sample_data <- list(n = nrow(datalogit),
                    K = ncol(Xdat),
                    x = as.matrix(Xdat),
                    y = datalogit$pobreza)
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
model_Losgit1 <- fitLosgit1$sample(data = sample_data, 
                 chains = 4,
                 parallel_chains = 4,
                 seed = 1234,
                 refresh = 1000)
```

La estimación del parámetro $B$ es:

```{r}
model_Losgit1$summary(variables = c("beta")) %>%
  select(variable:q95) %>% tba()
```

```{r}
mcmc_areas(model_Losgit1$draws("beta")) 
```

```{r}
mcmc_trace(model_Losgit1$draws("beta")) 
```

```{r}
y_pred_B <- model_Losgit1$draws(variables = "ypred", format = "matrix")
rowsrandom <- sample(nrow(y_pred_B), 100)
y_pred2 <- y_pred_B[rowsrandom, ]
ppc_dens_overlay(y = as.numeric(datalogit$pobreza), y_pred2)
```

Una alternativa es hacer el siguiente modelo que utiliza la función `bernoulli_logit`

Creando código de `STAN`

```{r, eval=FALSE}
data {
  int<lower=0> n;   // Número de observaciones
  int<lower=0> K;   // Número de predictores
  matrix[n, K] x;   // Matrix de predictores
  int<lower=0,upper=1> y[n];       // Vector respuesta
}
parameters {
  vector[K] beta;       // coefficients for predictors
 }
transformed parameters {
   vector[n] eta;
   eta = x * beta;
   }

model {
  to_vector(beta) ~ normal(0, 10000);
  y ~ bernoulli_logit(eta);  // likelihood
  }
generated quantities {
    real ypred[n];                    // vector de longitud n
    ypred = bernoulli_logit_rng(eta);
}


```

Preparando el código de `STAN`

```{r, eval=TRUE}
# file.edit("../Data/modelosStan/10ModeloLogit.stan")
fitLosgit2 <- cmdstan_model(stan_file = "../Data/modelosStan/10ModeloLogit.stan") 
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
model_Losgit2 <- fitLosgit2$sample(data = sample_data, 
                 chains = 4,
                 num_warmup = 1000,
                 num_samples = 1000,
                 parallel_chains = 4,
                 seed = 1234,
                 refresh = 1000)
```

La estimación del parámetro $B$ es:

```{r}
model_Losgit2$summary(variables = c("beta")) %>%
  select(variable:q95) %>% tba()
```

```{r}
mcmc_areas(model_Losgit2$draws("beta")) 
```

```{r}
mcmc_trace(model_Losgit2$draws("beta")) 
```

```{r}
y_pred_B <- model_Losgit2$draws(variables = "ypred", format = "matrix")
rowsrandom <- sample(nrow(y_pred_B), 100)
y_pred2 <- y_pred_B[rowsrandom, ]
ppc_dens_overlay(y = as.numeric(datalogit$pobreza), y_pred2)
```

## Modelo Binomial

En este caso la variable respuesta representa conteos de éxitos que se tuvieron en un conjunto de distintos experimentos. Se supone que $\mathbf{Y}=\{Y_1, \ldots, Y_n\}$ es un conjunto de variables aleatorias intercambiables cada una con distribución binomial de parámetro $\theta_i$ y $n_i$. El modelo binomial busca relacionar las probabilidades de éxito $\theta_i$ con variables auxiliares $\mathbf{X}_i$. Se considera a continuación la función de enlace logística.

La función de vínculo logística dada por $g(\theta_i)=\log\left(\frac{\theta_i}{1-\theta_i}\right)$, y denotando $\eta_i=\mathbf{X}_i'\boldsymbol{\beta}=g(\theta_i)$, se tiene que la verosimilitud de los datos está dada por

$$
\begin{align}
p(\mathbf{Y}\mid \boldsymbol{\theta})&=\prod_{i=1}^n\binom{n}{y_i}\theta_i^{y_i}(1-\theta_i)^{n-y_i} \notag \\
&=\prod_{i=1}^n\binom{n}{y_i}\left(\frac{\exp(\mathbf{X}_i'\boldsymbol{\beta})}{1+\exp(\mathbf{X}_i'\boldsymbol{\beta})}\right)^{y_i}
\left(1-\left(\frac{\exp(\mathbf{X}_i'\boldsymbol{\beta})}{1+\exp(\mathbf{X}_i'\boldsymbol{\beta})}\right)\right)^{n-y_i}
\end{align}
$$ Suponga que la distribución a previa para $\boldsymbol{\beta}$ está regida por la siguiente estructura probabilística

$$
\begin{equation*}
\boldsymbol{\beta}\sim N_q(\mathbf{b},\mathbf{B})
\end{equation*}
$$ De esta manera, la distribución a posterior toma la siguiente forma.

$$
\begin{align*}
p(\boldsymbol{\beta} \mid \mathbf{Y}, \mathbf{X})&\propto
\prod_{i=1}^n\left(\frac{\exp(\mathbf{X}_i'\boldsymbol{\beta})}{1+\exp(\mathbf{X}_i'\boldsymbol{\beta})}\right)^{y_i}
\left(1-\left(\frac{\exp(\mathbf{X}_i'\boldsymbol{\beta})}{1+\exp(\mathbf{X}_i'\boldsymbol{\beta})}\right)\right)^{n-y_i}\\
&\hspace{2cm}\times
\exp\left\{\frac{-1}{2}(\boldsymbol{\beta}-\mathbf{b})'\mathbf{B}^{-1}(\boldsymbol{\beta}-\mathbf{b})\right\}
\end{align*}
$$

Una vez más, la anterior expresión no tiene una forma cerrada.

### Práctica en `STAN`

Datos de la encuesta

```{r}
dataBino <- encuesta %>% 
  transmute(
    dam = str_pad(dam_ee, width = 2, pad = "0"),
    pobreza = ifelse(ingcorte < lp, 1, 0),
    ) %>% 
    group_by(dam) %>% 
  summarise(
    n = n() ,                  # Número de ensayos
    nPobreza = sum(pobreza)    # número de exitos 
    ) %>%                      # covariables  
  full_join(statelevel_predictors_df, by = "dam") 

tba(head(dataBino,10))
```

Creando código de `STAN`

```{r, eval=FALSE}
data {
  int<lower=0> D;   // Número de observaciones
  int<lower=0> K;   // Número de predictores
  int<lower=0> nd[D];  // Número de ensayos
  int<lower=0> yd[D];   // Número de exitos
  matrix[D, K] x;   // Matrix de predictores
}
parameters {
  vector[K] beta;       // coefficients for predictors
}
transformed parameters {
   vector[D] eta;
   eta =  x * beta;
   }

model {
  to_vector(beta) ~ normal(0, 10000);
  yd ~ binomial_logit(nd, eta);  // likelihood

  }
generated quantities {
    real ypred[D];                    // vector de longitud n
  ypred = binomial_rng(nd, inv_logit(eta));  

}

```

Preparando el código de `STAN`

```{r, eval=TRUE}
# file.edit("../Data/modelosStan/11ModeloBinomial.stan")
fitBinomial <- cmdstan_model(stan_file = "../Data/modelosStan/11ModeloBinomial.stan") 
```

Organizando datos para `STAN`

```{r}
Xdat <- model.matrix(nPobreza ~  material_paredes +
                      alfabeta +  tiene_internet + 
                      piso_tierra + piso_tierra +
                       rezago_escolar  + tiene_alcantarillado +
                       tiene_electricidad ,
                     data = dataBino)

tba(Xdat)
```

```{r}
sample_data <- list(D = nrow(dataBino),
                    K = ncol(Xdat),
                    x = as.matrix(Xdat),
                    yd = dataBino$nPobreza,
                    nd = dataBino$n)
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
model_Binomial <- fitBinomial$sample(
                 data = sample_data, 
                 chains = 4,
                 parallel_chains = 4,
                 seed = 1234,
                 refresh = 1000)
```

La estimación del parámetro $B$ es:

```{r}
model_Binomial$summary(variables = c("beta")) %>%
  select(variable:q95) %>% tba()
```

```{r}
mcmc_areas(model_Binomial$draws("beta")) 
```

```{r}
mcmc_trace(model_Binomial$draws("beta")) 
```

```{r}
y_pred_B <- model_Binomial$draws(variables = "ypred", format = "matrix")
rowsrandom <- sample(nrow(y_pred_B), 100)
y_pred2 <- y_pred_B[rowsrandom, ]
ppc_dens_overlay(y = as.numeric(dataBino$nPobreza), y_pred2)
```

## Regresión multinomial con vínculo logit

La distribución multinomial es una extensión de la distribución binomial, esta se aplica en situaciones donde hay un número fijo de ensayos independientes, donde cada el resultado de cada ensayo corresponde a una de $K$ categorías.

Generalizando lo anterior, se considera $\mathbf{Y}=\{\mathbf{Y}_1, \ldots, \mathbf{Y}_n\}$ un conjunto de vectores aleatorios intercambiables cada una con distribución multinomial de parámetros $(n_i, \boldsymbol{\theta}_i)$. Note que todos lo vectores tienen $K$ categorias; en particular, el $i$-ésimo vector del conjunto se define como $\mathbf{Y}_i=(Y_{i1}, \ldots, Y_{iK})'$, el vector de parámetros de interés es $\boldsymbol{\theta}_i=(\theta_{i1},\ldots, \theta_{iK})$ con $\sum_{k=1}^K\theta_{ik}=1$, y $n_i=\sum_{k=1}^K Y_{ik}$, y por ende se tiene que la distribución, condicional a $n_i$, de $\mathbf{Y}_{i}$ sigue una distribución multinomial tal que:

$$
\begin{equation}
p(\mathbf{Y}_{i}\mid n_i, \boldsymbol{\theta}_i)=\binom{n_i}{y_{i1}, \ldots, y_{iK}}\prod_{k=1}^K\theta_{ik}^{y_{ik}}
\end{equation}
$$

Con base en lo anterior, la verosimilitud de los datos se tiene mediante la siguiente expresión

$$
\begin{equation}
p(\mathbf{Y} \mid n, \boldsymbol{\theta})=\prod_{i=1}^n \binom{n_i}{y_{i1}, \ldots, y_{iK}}\prod_{k=1}^K\theta_{ik}^{y_{ik}}
\end{equation}
$$

Antes de proseguir con el modelamiento bayesiano, es útil notar que en este caso el vínculo no es un vector sino una matriz que responde a una relación lineal entre las covariables y una matriz de coeficientes de regresión, como se puede ver a continuación.

$$
\begin{equation*}
\begin{bmatrix}
  \eta_{11} & \eta_{12} & \cdots & \eta_{1K} \\
  \vdots & \vdots & \ddots & \vdots \\
  \eta_{n1} & \eta_{n2} & \cdots & \eta_{nK}
\end{bmatrix}
=
\begin{bmatrix}
  X_{11} & X_{12} & \cdots & X_{1q} \\
  \vdots & \vdots & \ddots & \vdots \\
  X_{n1} & X_{n2} & \cdots & X_{nq}
\end{bmatrix}
\begin{bmatrix}
  \beta_{11} & \beta_{12} & \cdots & \beta_{1K} \\
  \vdots & \vdots & \ddots & \vdots \\
  \beta_{q1} & \beta_{q2} & \cdots & \beta_{qK}
\end{bmatrix}
\end{equation*}
$$

Es decir, 

$$
\begin{equation}
\boldsymbol{\eta}=\mathbf{X}'\boldsymbol{\beta}
\end{equation}
$$

Lo anterior conlleva a que $\eta_{ik}=\mathbf{X}_i'\boldsymbol{\beta}_k$, donde $\mathbf{X}_i$ es la $i$-ésima fila de la matriz $\mathbf{X}$ y $\boldsymbol{\beta}_k$ es la $k$-ésima columna de la matriz $\boldsymbol{\beta}$. Ahora, tomando como línea de base la primera columna de la matriz $\boldsymbol{\eta}$ (es decir, el vector de las probabilidades de la categoría 1 para los $n$ individuos), y utilizando la función de vinculo logístico, se tiene que para los elementos en las restantes columnas de $\boldsymbol{\eta}$,

$$
\begin{equation}
\eta_{ik}=g(\theta_{ik})=\log\left(\frac{\theta_{ik}}{\theta_{i1}}\right)=\log\left(\frac{\theta_{ik}}{\theta_{i1}}\right)
\end{equation}
$$ Con un poco de álgebra se comprueba que la función inversa para $g(\cdot)$ está dada por la siguiente expresión 

$$
\begin{equation}
\theta_{ik}=g^{-1}(\eta_{ik})=\theta_{i1}\exp(\eta_{ik}) \ \ \ \ \ \forall k=2,\ldots,K
\end{equation}
$$

Ahora, para la primera columna de $\boldsymbol{\eta}$, es decir, el vector de probabilidades de la primera categoría, se tiene que

$$
\begin{equation*}
\theta_{i1}=1-\sum_{j=2}^K\theta_{ij}=1-\sum_{j=2}^K\theta_{i1}\exp(\eta_{ij})
\end{equation*}
$$

de donde se tiene que

$$
\begin{equation}
\theta_{i1}=\frac{1}{1+\sum_{j=2}^K\exp(\eta_{ij})}
\end{equation}
$$

Finalmente, se tiene que $$
\begin{equation*}
\theta_{ik}=\frac{\exp(\eta_{ik})}{1+\sum_{k=2}^K\exp(\eta_{ik})} \ \ \ \ \forall k=1,\ldots,K
\end{equation*}
$$

con $\boldsymbol{\beta}_1=0$, esto es $\eta_{i1}=0$ para todo $i$. Notando que $\eta_{ik}=\mathbf{X}_i'\boldsymbol{\beta}_k$, se tiene que la verosimilitud de los datos toma la siguiente forma

$$
\begin{align*}
p(\mathbf{Y} \mid n, \boldsymbol{\theta})=&\prod_{i=1}^n \binom{n_i}{y_{i1}, \ldots, y_{iK}}\prod_{k=1}^K\theta_{ik}^{y_{ik}}\\
p(\mathbf{Y} \mid n, \boldsymbol{\beta})=&\prod_{i=1}^n \binom{n_i}{y_{i1}, \ldots, y_{iK}}
\prod_{k=1}^K\left(\frac{\exp(\mathbf{X}_i'\boldsymbol{\beta}_k)}{1+\sum_{k=2}^K\exp(\mathbf{X}_i'\boldsymbol{\beta}_k)}\right)^{y_{ik}}\\
\end{align*}
$$

Suponiendo que la distribución a priori para $\boldsymbol{\beta}_k$ está regida por la siguiente estructura probabilística $$
\begin{equation*}
\boldsymbol{\beta}_k\sim N_q(\mathbf{b}_k,\mathbf{B}_k)
\end{equation*}
$$

De esta manera, la distribución a posteriori para el $k$ ésimo vector $\boldsymbol{\beta}_k$ toma la siguiente forma.

$$
\begin{align*}
p(\boldsymbol{\beta}_k \mid \mathbf{Y}, \mathbf{X}, n)&\propto
\prod_{i=1}^n \left(\frac{\exp(\mathbf{X}_i'\boldsymbol{\beta}_k)}{1+\sum_{k=2}^K\exp(\mathbf{X}_i'\boldsymbol{\beta}_k)}\right)^{y_{ik}}\\
&\hspace{2cm}\times
\exp\left\{-\frac{1}{2}(\boldsymbol{\beta}-\mathbf{b})'\mathbf{B}^{-1}(\boldsymbol{\beta}-\mathbf{b})\right\}
\end{align*}
$$

Una vez más, la anterior expresión no tiene una forma cerrada.

### Práctica en `STAN`

Datos de la encuesta

```{r}
dataMultinomial <- encuesta %>%
  filter(edad >= 18, condact3 %in% 1:3) %>% 
  transmute(
       dam = str_pad(dam_ee, width = 2, pad = "0"),
    empleo = as_factor(condact3),
    ) %>% 
    group_by(dam, empleo) %>% 
   tally() %>% data.frame()

dataMultinomial <- dataMultinomial %>%
  spread(key = "empleo",value = "n", fill = 0) %>% 
 inner_join(statelevel_predictors_df, by = "dam")
dataMultinomial <-
  dataMultinomial %>% rename(Ocupado = "1",
                             Desocupado = "2",
                             Inactivo = "3")
tba(dataMultinomial)
```

Creando código de `STAN`

```{r, eval=FALSE}
data {
  int<lower=1> D;    // Número de dominios 
  int<lower=1> P;    // Categorías
  int<lower=1> K;    // Número de regresores
  int y[D, P];       // Matriz de datos
  matrix[D, K] X;    // Matriz de covariables
}
  

parameters {
  matrix[P-1, K] beta;// Matriz de parámetros 
                 
}

transformed parameters {
  simplex[P] theta[D];// Vector de parámetros;
  real num[D, P];
  real den[D];

for(d in 1:D){
    num[d, 1] = 1;
    num[d, 2] = exp(X[d, ] * beta[1, ]') ;
    num[d, 3] = exp(X[d, ] * beta[2, ]') ;
    
    den[d] = sum(num[d, ]);

  }
  for(d in 1:D){
    for(p in 2:P){
    theta[d, p] = num[d, p]/den[d];
    }
    theta[d, 1] = 1/den[d];
  }
}

model {
  to_vector(beta) ~ normal(0, 100);
  for(d in 1:D){
    target += multinomial_lpmf(y[d, ] | theta[d, ]); 
  }
}

generated quantities {
    int ypred[D,P];               
    for(d in 1:D){
    ypred[d,] = multinomial_rng(theta[d, ], sum(y[d, ])); 
  }
}

```

Preparando el código de `STAN`

```{r, eval=TRUE}
# file.edit("../Data/modelosStan/12ModeloMultinomial.stan")
fitMultinomial <- cmdstan_model(stan_file = "../Data/modelosStan/12ModeloMultinomial.stan") 
```

Organizando datos para `STAN`

```{r}
Xdat <- model.matrix(
  Ocupado ~ tasa_desocupacion + 
    suelo_cultivos  + 
    suelo_urbanos + 
    alfabeta + 
    rezago_escolar  ,
  data = dataMultinomial
)

tba(Xdat)
```

```{r}
ydat <- dataMultinomial %>%
  select(Ocupado, Desocupado, Inactivo) %>% 
  as.matrix()
tba(ydat)
```

```{r}
sample_data <- list(D = nrow(dataMultinomial),
                    K = ncol(Xdat),
                    X = as.matrix(Xdat),
                    P = ncol(ydat),
                    y = ydat
                    )
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
# Tiempo 5 minutos 
model_Multinomial <- fitMultinomial$sample(
                 data = sample_data, 
                 num_samples = 1000,
                 num_warmup = 1000,
                 chains = 4,
                 parallel_chains = 4,
                 seed = 1234,
                 refresh = 1000)
```

La estimación del parámetro $B$ es:

```{r}
model_Multinomial$summary(variables = c("beta")) %>%
  select(variable:q95) %>% tba()
```

```{r}
mcmc_areas(model_Multinomial$draws("beta")) 
```

```{r, out.height= "200%"}
mcmc_trace(model_Multinomial$draws("beta")) 
```

La predicción del número de persona en cada estado se obtiene como:

```{r}
ydat2 <- as.vector(ydat)
y_pred_B <- model_Multinomial$draws(variables = "ypred", format = "matrix")
rowsrandom <- sample(nrow(y_pred_B), 100)
y_pred2 <- y_pred_B[rowsrandom, ]

ppc_dens_overlay(y = ydat2, 
                 y_pred2)


```

Para la probabilidad de pertenecer al estado $k-esimo$ se obtiene como:

```{r}
ydat2 <- as.vector(ydat/rowSums(ydat))
y_pred_B <- model_Multinomial$draws(variables = "theta", format = "matrix")
rowsrandom <- sample(nrow(y_pred_B), 100)
y_pred2 <- y_pred_B[rowsrandom, ]

ppc_dens_overlay(y = ydat2, 
                 y_pred2)


```

